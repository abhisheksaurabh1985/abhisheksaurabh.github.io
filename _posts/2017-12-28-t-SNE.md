---
layout: post
title:  "t-distributed Stochastic Neighbor Embedding (t-SNE)"
date:   2017-12-29 13:23:07 +0100
categories: general
---
Yesterday, I read the paper on t-SNE. I found the idea quite fascinating and thought to share my understanding with everyone else. Jupyter notebook accompanying this post can be found __[here](https://github.com/abhisheksaurabh1985/yet-another-ml-tutorial/blob/master/t-SNE.ipynb)__.   

## Introduction
Visualization of high dimensional data is a key to problem solving in multiple domains. As an example, in Bioinformatics, cell nuclei which is relevant to breast cancer, is described by approximately 30 variables. Likewise, image analysis or word embeddings in natural language processing typically have thousands of dimensions. Visualization of such a high dimensional space is an active area of research. 

t-SNE is one of the many machine learning algorithms which was invented for visualization of high dimensional data in a lower dimensional space. As is the goal of similar techniques, the aim is to preserve much of the significant structure of the high dimensional data, as possible in the low dimensional map.  

In this notebook, I have left out the mathematical details of the algorithm. The original paper, link to which is shared in the reference above, is lucid enough to understand it in detail. Nevertheless, I'll explain the idea behind the algorithm.  

## t-SNE
The algorithm comprises of two main steps. First, it defines a probability distribution (usually a normal distribution) in the high dimensional space s.t. the data points which are similar in 'some' respect, have a higher probability of being picked. Second, it defines another probability distribution (`t-distribution`, hence the name) over the data points in the lower dimensional space. It then minimizes the `KL Divergence` between the two distributions w.r.t. to the locations of the points in the map. 

Let's consider $N$ high dimensional points $\mathbf {x} _{1},\dots ,\mathbf {x} _{N}$. t-SNE first computes the conditional probability $p_{j\mid i}$ that point $x_{i}$ would pick $x_{i}$ as its neighbour, if neighbours are picked in proportion to their probability density under a Gaussian centered at $x_{i}$. The conditional probability is given by: 

$\displaystyle p_{j\mid i}={\frac {\exp(-\lVert \mathbf {x} _{i}-\mathbf {x} _{j}\rVert ^{2}/2\sigma _{i}^{2})}{\sum _{k\neq i}\exp(-\lVert \mathbf {x} _{i}-\mathbf {x} _{k}\rVert ^{2}/2\sigma _{i}^{2})}}$

Let $\mathbf {y} _{1},\dots ,\mathbf {y} _{N}$, represent the lower (say _d_) dimensional map of the points. The algorithm tries to learn this mapping in such a way that the resulting distribution in the lower dimensional space reflects the similarities defined by $p_{j\mid i}$ as well. More specifically, $q_{j\mid i}$ is given by the following expression. 

$\displaystyle q_{ij}={\frac {(1+\lVert \mathbf {y} _{i}-\mathbf {y} _{j}\rVert ^{2})^{-1}}{\sum _{k\neq i}(1+\lVert \mathbf {y} _{i}-\mathbf {y} _{k}\rVert ^{2})^{-1}}}$

It must be noted that while a normal distribution is used to model the data points in the original space, a `t-student distribution` is used to map the points in the lower dimensional space. 

The algorithm then determines the location of the points in the map space by minimizing the `KL divergence` between the distribution $P$ and $Q$ using `gradient descent`.  
